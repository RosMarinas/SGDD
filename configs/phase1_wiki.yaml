# Phase 1 Configuration: Wikipedia Text Reconstruction
# 任务: 输入文本 → 编码 → 解码 → 相同文本
# HF_ENDPOINT=https://hf-mirror.com python src/train.py --config configs/phase1_wiki.yaml

# 模型配置
model:
  encoder_name: "roberta-base"
  encoder_freeze: true
  semantic_dim: 512  # 对应 SGDDConfig 的 hidden_dim
  
  # VIB settings
  kl_weight: 0.1        # Increased from 0.001 to 0.1 for stronger regularization
  kl_anneal_steps: 100  # Reduced from 10000 to 100 to kick in during short training
  kl_threshold: 2.0     # Free bits

  # Decoder 配置 (字段名与 SGDDConfig 保持一致)
  num_layers: 6
  num_heads: 8
  ffn_dim: 2048
  max_length: 128  # 对应 SGDDConfig 的 max_len
  dropout: 0.1  # 对应 SGDDConfig 的 dropout

  num_diffusion_steps: 1000
  noise_schedule: "cosine"

  # 训练优化配置 (字段名与 SGDDConfig 保持一致)
  use_self_conditioning: true  # 启用self-conditioning提升质量
  compute_pad_loss: false  # MaskGIT标准:只在MASK位置计算loss,不计算PAD
  compute_eos_loss: true  # 启用EOS token学习，支持变长输出
  word_dropout_prob: 0.3  # Force decoder to use semantic vector z (30%-50% recommended)

# 训练配置
training:
  learning_rate: 0.0002
  weight_decay: 0.01
  batch_size: 16  # Reduced batch size for safety
  num_epochs: 2   # Reduced epochs
  gradient_accumulation_steps: 1
  grad_clip: 1.0

  lr_scheduler: "cosine"
  warmup_steps: 100  # Reduced warmup
  use_fp16: true
  cfg_drop_prob: 0.1

  log_interval: 10
  save_interval: 100
  eval_interval: 100
  save_epochs: 1

  use_wandb: true
  wandb_project: "sgdd"
  wandb_entity: null
  wandb_run_name: "phase1_wiki_debug"

# 数据配置
data:
  dataset: "wikipedia"

  wiki_num_samples: 1000  # Reduced sample count
  wiki_min_length: 20
  wiki_max_length: 128

  num_workers: 4
  pin_memory: true

  val_split: 0.1  # Increased validation split for small data

# 推理配置
inference:
  num_inference_steps: 16  # 优化: MaskGIT通常16步足够,256步过高
  temperature: 1.0
  cfg_scale: 2.0
  sampling_strategy: "confidence"
  top_k: null
  top_p: null

# 系统配置
seed: 42
device: "cuda"
checkpoint_dir: "checkpoints/phase1_wiki"
log_dir: "logs/phase1_wiki"
task: "reconstruction"
