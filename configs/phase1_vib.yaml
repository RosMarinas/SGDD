# Phase 1 Training with Variational Information Bottleneck
# This config enables VIB to address RoBERTa space anisotropy issues

# 模型配置
model:
  encoder_name: "BAAI/bge-base-en-v1.5"  # 使用 BGE Base 作为编码器
  encoder_freeze: true
  semantic_dim: 768  # Base model dimension
  decoder_dim: 768    # Matching decoder dimension (no reduction)
  
  # VIB Configuration (now default behavior)
  kl_weight: 0.05  # Reduced from 0.2 to avoid early KL dominance
  kl_anneal_steps: 8000  # Anneal KL weight over first 10K steps

  # Decoder 配置
  num_layers: 4
  num_heads: 12  # 768 / 64 = 12 heads
  ffn_dim: 3072  # 4 * 768
  max_length: 64
  dropout: 0.15

  # Diffusion 配置

  num_diffusion_steps: 1000
  noise_schedule: "cosine"

  # 训练优化配置
  use_self_conditioning: true
  compute_pad_loss: false
  compute_eos_loss: true

# 训练配置
training:
  learning_rate: 0.0004
  weight_decay: 0.01
  batch_size: 256
  num_epochs: 4
  gradient_accumulation_steps: 1
  grad_clip: 1.0

  lr_scheduler: "cosine"
  warmup_steps: 5000  # 减少到 ~1 个 epoch

  use_fp16: true
  cfg_drop_prob: 0.15  # 15% unconditional batches for CFG

  log_interval: 20
  eval_interval: 1000
  save_interval: 1000
  save_epochs: 0 # 禁用按 epoch 保存，改用按 step 滚动保存

  use_wandb: true
  wandb_project: "sgdd-vib-bge"
  wandb_entity: null
  wandb_run_name: "phase1_vib_bge_base_experiment"

# 数据配置
data:
  dataset: "bookcorpus"
  dataset_path: "data/BookCorpus/final_dataset_1.4B"
  
  # 数据处理
  max_token_length: 64
  min_length: 5

  num_workers: 4
  pin_memory: true
  val_split: 0.03

# 推理配置
inference:
  # 推理步数：16-32 步即可（MaskGIT 风格快速推理）
  # 即使 num_diffusion_steps=256，只需要 16 步迭代就能生成好结果
  num_inference_steps: 16
  temperature: 1.0
  cfg_scale: 2.0
  sampling_strategy: "confidence"
  top_k: null
  top_p: null

# 系统配置
seed: 42
device: "cuda"
checkpoint_dir: "checkpoints/phase1_vib_bge_base"
log_dir: "logs/phase1_vib_bge_base"
task: "reconstruction"
