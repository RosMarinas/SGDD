# Phase 1 Training with Variational Information Bottleneck
# This config enables VIB to address RoBERTa space anisotropy issues

# 模型配置
model:
  encoder_name: "roberta-base"
  encoder_freeze: true
  semantic_dim: 512

  # VIB Configuration (now default behavior)
  kl_weight: 0.2  # Conservative KL weight
  kl_anneal_steps: 800  # Anneal KL weight over first 10K steps

  # Decoder 配置
  num_layers: 2
  num_heads: 8
  ffn_dim: 2048
  max_length: 128
  dropout: 0.1

  # Diffusion 配置
  # 使用 256 个时间步的噪声调度（训练和推理都用这个范围）
  num_diffusion_steps: 1000
  noise_schedule: "cosine"

  # 训练优化配置
  use_self_conditioning: true
  compute_pad_loss: false
  compute_eos_loss: true

# 训练配置
training:
  learning_rate: 0.00015
  weight_decay: 0.01
  batch_size: 64
  num_epochs: 50
  gradient_accumulation_steps: 2
  grad_clip: 1.0

  lr_scheduler: "cosine"
  warmup_steps: 100  # 减少到 ~1 个 epoch

  use_fp16: true
  cfg_drop_prob: 0.15  # 15% unconditional batches for CFG

  log_interval: 20
  save_interval: 1000
  save_epochs: 10

  use_wandb: true
  wandb_project: "sgdd-vib"
  wandb_entity: null
  wandb_run_name: "phase1_vib_experiment"

# 数据配置
data:
  dataset: "mixed"
  mixing_strategy: "fast_validation"
  total_samples: 100

  # Wikipedia 配置
  wiki_num_samples: 6000
  wiki_min_length: 20
  wiki_max_length: 128
  wiki_max_token_length: 128  # 添加这个参数以匹配模型 max_length

  # Alpaca 配置
  alpaca_num_samples: 3000
  alpaca_min_length: 20
  alpaca_max_token_length: 128  # 添加这个参数

  # OASST1 配置
  oasst1_num_samples: 1000
  oasst1_min_length: 20
  oasst1_max_token_length: 128  # 添加这个参数

  num_workers: 4
  pin_memory: true
  val_split: 0.03

# 推理配置
inference:
  # 推理步数：16-32 步即可（MaskGIT 风格快速推理）
  # 即使 num_diffusion_steps=256，只需要 16 步迭代就能生成好结果
  num_inference_steps: 16
  temperature: 1.0
  cfg_scale: 2.0
  sampling_strategy: "confidence"
  top_k: null
  top_p: null

# 系统配置
seed: 42
device: "cuda"
checkpoint_dir: "checkpoints/phase1_vib"
log_dir: "logs/phase1_vib"
task: "reconstruction"
