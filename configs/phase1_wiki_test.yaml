# Phase 1 Test Configuration: Wikipedia Text Reconstruction (1 Epoch)
# 任务: 输入文本 → 编码 → 解码 → 相同文本

# 模型配置
model:
  encoder_name: "roberta-base"
  encoder_freeze: true
  semantic_dim: 512

  decoder_layers: 6
  decoder_hidden: 512
  decoder_heads: 8
  decoder_ffn_dim: 2048
  max_length: 64

  num_diffusion_steps: 1000
  noise_schedule: "cosine"

# 训练配置
training:
  learning_rate: 0.001
  weight_decay: 0.01
  batch_size: 64
  num_epochs: 1  # 只训练1个epoch用于测试
  gradient_accumulation_steps: 1
  grad_clip: 1.0

  lr_scheduler: "cosine"
  warmup_steps: 100  # 减少warmup步数

  use_fp16: true
  cfg_drop_prob: 0.1

  log_interval: 10
  save_interval: 500  # 更频繁地保存
  eval_interval: 200   # 更频繁地评估
  save_epochs: 5

  use_wandb: true
  wandb_project: "sgdd"
  wandb_entity: null
  wandb_run_name: "phase1_wiki_test_1epoch"

# 数据配置
data:
  dataset: "wikipedia"

  wiki_num_samples: 1000  # 只用1000个样本用于测试
  wiki_min_length: 20
  wiki_max_length: 200

  num_workers: 0  # Windows下设置为0避免multiprocessing问题
  pin_memory: false

  val_split: 0.05

# 推理配置
inference:
  num_inference_steps: 16
  temperature: 1.0
  cfg_scale: 2.0
  sampling_strategy: "confidence"
  top_k: null
  top_p: null

# 系统配置
seed: 42
device: "cuda"
checkpoint_dir: "checkpoints/phase1_wiki_test"
log_dir: "logs/phase1_wiki_test"
task: "reconstruction"
